День 25. Запустить локальную модель
Установите и запустите любую LLM локально (Ollama, LM Studio, Ollama + LLaMA, Mistral, Phi, Qwen и т.п.)
Проверьте, что вы можете к ней обращаться через CLI или API
Попробуйте задать ей вопрос и получить ответ
Результат: локальная LLM запущена и отвечает на запросы
Формат: Видео + Код